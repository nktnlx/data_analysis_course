Hi there!


I'm learning to code, interested in data analytics and data science, aiming to be hired by the mid of 2021.  
This repository is for [data analysis course](https://karpov.courses/) I've enrolled in January 2021.    

The course curriculum includes the following technologies and topics mastered by me:  
1. Python
2. Pandas
3. Numpy
4. Seaborn
5. Google APIs
6. Git
7. Airflow
8. SQL
9. ClickHouse
10. PostgreSQL
11. Redash
12. Superset
13. Statistics
14. A/B-tests
15. Bootstrapping
16. Power Analysis
17. Tableau
18. DAU, MAU, ARPU, LTV, Retention, CR and other metrics
19. Product Development basics
20. Product Management basics
21. Soft-skills  
   
   
   
   
   
**List of projects:**
1. **Taxi in NYC** -- analising NYC taxi orders with Pandas. Read_csv, rename, groupby, agg, query, sort_values, idxmax, idxmin, value_counts, pivot methods were used for Exploratory Data Analysis.
2. **Hotel Bookings** -- analising hotel bookings with Pandas. Read_csv, info, rename, groupby, agg, query, sort_values, idxmax, idxmin, value_counts, pivot methods were used for Exploratory Data Analysis. Customers **Churn rate** was calculated.  
3. **User Logs** -- analising customers data. Finding the most popular platform and the most active users. Visualizing data with Seaborn distplot, barplot and countplot methods.       
4. **Taxi in Peru** -- analising  taxi orders in Peru with Pandas. An Exploratory Data Analysis was performed. Drivers' score, passengers' score, **DAU** and **MAU** metrics were calculated and plotted with Seaborn.   
5. **Raw Data Handling** -- creating dataframe from a set of csv-files stored in various folders. Practicing Python skills to automate data handling.  
6. **Retail in Germany** -- having a dataset with purchases of clients from Europe. Count basic sales statistics for clients from Germany. Duplicated, drop_duplicates, groupby, agg, query, sort_values, assign, quantile and str methods were used for Exploratory Data Analysis. 
7. **Error in Transactions Data** -- we've found and corrected an error while analising a dataset with transactions. Plotting data in logarithmic scale, converting data to datetime format as well as implementing describe, isna, sum, value_counts, groupby, agg, query, sort_values, rename, min, max and pivot methods were used for Exploratory Data Analysis.   
8. **Avocado Price** -- comparing avocado average, simple moving average and exponential weighted average price values. Categorizing delay data and labeling it. Plotting results with help of subplots and interactive Plotly plots.  
9. **Ads Campaign** -- plotting data in logarithmic scale to find the type of data distribution, finding ad_id with an anomalistic number of views. Comparing average and simple moving average views data. Calculating clients' registration to publishing ad conversion rate (**CR**). Categorizing clients' registration data and labeling it. Plotting results with help of interactive Plotly plot.  
10. **Visits by Browser** -- analising web-site visits. Defining proportion of real users and visits by bots. Finding the most popular browser for users and for bots. Bar-plotting results, downloading data using **Google Docs API** and merging it to our dataframe. Read_csv, groupby, agg, query, sort_values, pivot, fillna, assign and merge methods were used for Exploratory Data Analysis.
11. **Telegram Bot Airflow Reporting**  -- reading an advertising campaign data from Google Docs spreadsheet, creating pandas dataframe to calculate clicks, views, **CTR** and money spent on the campaign. Calculating day by day change of the metrics, writing report with results to a txt file and sending this file via telegram bot to your mobile phone. The script is executed by Airflow every Monday at 12:00 p.m.   
12. **SQL Tasks** -- SQL exercises done by me while passing this data analysis course. Clickhouse (via Tabix) was used to solve the tasks.  
13. **NYC taxi & timeit optimization** -- calculating distance of a ride using pick-up and drop-off coordinates. Compared a couple of ways to apply distance calculation to the dataframe. The optimization helped to decrease calculation run-time about 3276 times! Checked calculation results, found outliers using boxplot graphs and descriptive statistics. Fixed dataframe by removing outliers and found the cost of the longest ride.  
14. **Bikes rent in Chicago** -- dates to dateformat conversion, resampling data to aggregate by days, automatically merging data from distinct files into one dataframe using os.walk(), differentiating bikes rents by user type, finding the most popular destination points overall and based on the week of the day.  
15. **Bookings in London** -- used Pandahouse and SQL queries to import data from Clickhouse into pandas dataframe. Processed imported data and performed Exploratory Data Analysis. Built scatterplot, distplot, lineplot and heatmap using Seaborn and Matplotlib.  
16. **Retail dashboard** -- built a series of visualizations and a dashboard using SQL queries and Redash. Calculated and checked the dynamics of **MAU** and **AOV**. Found an anomaly in data, defined the market generating the majority of revenue, analyzed the most popular goods sold in the store. Wrote a dashboard summary with recommendation to push up sales.
17. **Video games** --  analising video games sales dynamics with Pandas. Read_csv, head, columns, dtypes, info, isna, dropna, describe, mode, shape, groupby, agg, sort_values, rename, index, to_list, value_counts methods were user for Exploratory Data Analysis. Barplot, boxplot and lineplot were used for graphing results.    
18. **Ads conversion** -- calculating **CTR, CPC, CR** metrics. Plotting them using distplot, hist, displot and histplot methods.
19. **Yandex Music** -- analyzing music streaming platform songs popularity, comparing music preferences and listening templates in Moscow and Saint-Petersburg. Reading and cleaning data, renaming columns, removing duplicates, dealing with missing data, slicing dataframe to query required portion of data.  
20. **Bikes rent in London** -- loading dataset, plotting rides count data, resampling timestamps, describing the main trends, looking for anomaly values by smoothing the data with a **simple moving average**, calculating the difference between the real data and smoothed data, finding **standard deviation** and defining the **99% confidence interval**. Then we compare values with the confidence interval to find data hikes and explain them.  
21. **Delivery A/B** -- Finding how a new navigational algorithm has changed the delivery time of the service. Formulating null and alternative hypothesis and performing A/B test with help of t-test.  
22. **App interface A/B** -- Testing how images aspect ratio and a new order button design influence on the amount of orders placed by customers. Performed Levene's test to check group variance equality, Shapiro-Wilk test to check groups for normality, one-way ANOVA to check statistically significant difference between tested groups, Tukey's test to find statistically significant difference between groups, linear model multivariate analysis of variance, visualized and interpreted results, gave recommendations to put (or not to put) changes into production.  
23. **Cars sales** -- predicting cars sales price using linear regerssion models (statsmodels.api & statsmodels.formula.api). Finding statistically significant predictors.  
24. **Bootstrap A/B** -- comparing results of Mann-Whitney test and Bootstrap mean/median running on data with and without outliers.  




<br>
Hope this repo will help you to assess my coding, data analytics and SQL skills or will be just fun for you to look through.    



--------------------------------------------
Fill free to contact me via nktn.lx@gmal.com  
Follow me on twitter: @nktn_lx  
And here on github: github.com/nktnlx  